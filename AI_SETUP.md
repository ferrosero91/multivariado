# ü§ñ Configuraci√≥n del Sistema Multi-IA

EasyCal Pro utiliza un sistema inteligente de m√∫ltiples IAs con fallback autom√°tico para resolver problemas matem√°ticos. Si una IA falla, el sistema autom√°ticamente prueba la siguiente.

## üöÄ Proveedores de IA Disponibles

### 1. **Groq** (Recomendado) ‚ö°
- **Velocidad**: Ultra-r√°pido (< 1 segundo)
- **Precisi√≥n**: Muy alta
- **Modelos**: Llama 3.1, Llama 3.2, Gemma 2
- **Costo**: Gratuito con l√≠mites generosos
- **Setup**: Requiere API key

**C√≥mo obtener API key:**
1. Ve a [console.groq.com](https://console.groq.com/)
2. Crea una cuenta gratuita
3. Ve a "API Keys" y crea una nueva key
4. Agrega `NEXT_PUBLIC_GROQ_API_KEY=gsk_tu_key_aqui` a tu `.env.local`

### 2. **OpenRouter** üåê
- **Velocidad**: R√°pido
- **Precisi√≥n**: Alta
- **Modelos**: Llama 3.2, Phi-3, Gemma 2, Qwen 2
- **Costo**: Modelos gratuitos disponibles
- **Setup**: Opcional (funciona sin API key)

**C√≥mo obtener API key (opcional):**
1. Ve a [openrouter.ai](https://openrouter.ai/)
2. Crea una cuenta
3. Ve a "Keys" y crea una nueva key
4. Agrega `NEXT_PUBLIC_OPENROUTER_API_KEY=sk-or-tu_key_aqui` a tu `.env.local`

### 3. **Hugging Face** ü§ó
- **Velocidad**: Moderado
- **Precisi√≥n**: Buena
- **Modelos**: DialoGPT, BlenderBot, FLAN-T5
- **Costo**: Gratuito
- **Setup**: Opcional (funciona sin API key)

**C√≥mo obtener token (opcional):**
1. Ve a [huggingface.co](https://huggingface.co/)
2. Crea una cuenta
3. Ve a Settings > Access Tokens
4. Crea un token de lectura
5. Agrega `NEXT_PUBLIC_HUGGINGFACE_API_KEY=hf_tu_token_aqui` a tu `.env.local`

### 4. **Cohere** üîÆ
- **Velocidad**: R√°pido
- **Precisi√≥n**: Buena
- **Modelos**: Command Light
- **Costo**: Tier gratuito disponible
- **Setup**: Autom√°tico (usa demo key)

### 5. **Ollama Local** üè†
- **Velocidad**: Depende del hardware
- **Precisi√≥n**: Alta
- **Modelos**: Llama 3.2, Mistral, CodeLlama
- **Costo**: Completamente gratuito
- **Setup**: Requiere instalaci√≥n local

**C√≥mo instalar Ollama:**
1. Ve a [ollama.ai](https://ollama.ai/)
2. Descarga e instala Ollama
3. Ejecuta: `ollama pull llama3.2:3b`
4. Inicia el servidor: `ollama serve`

### 6. **Local Fallback** üõ†Ô∏è
- **Velocidad**: Instant√°neo
- **Precisi√≥n**: B√°sica
- **Modelos**: L√≥gica matem√°tica integrada
- **Costo**: Gratuito
- **Setup**: Siempre disponible

## ‚öôÔ∏è Configuraci√≥n R√°pida

### Opci√≥n 1: Solo Groq (Recomendado)
```bash
# .env.local
NEXT_PUBLIC_GROQ_API_KEY=gsk_tu_groq_key_aqui
```

### Opci√≥n 2: Configuraci√≥n Completa
```bash
# .env.local
NEXT_PUBLIC_GROQ_API_KEY=gsk_tu_groq_key_aqui
NEXT_PUBLIC_OPENROUTER_API_KEY=sk-or-tu_openrouter_key_aqui
NEXT_PUBLIC_HUGGINGFACE_API_KEY=hf_tu_huggingface_token_aqui
```

### Opci√≥n 3: Sin API Keys (Solo gratuito)
No necesitas configurar nada. El sistema usar√°:
- OpenRouter (modelos gratuitos)
- Hugging Face (API gratuita)
- Cohere (demo key)
- Local Fallback

## üîÑ C√≥mo Funciona el Sistema de Fallback

1. **Prioridad 1**: Groq (si est√° configurado)
2. **Prioridad 2**: OpenRouter (modelos gratuitos)
3. **Prioridad 3**: Hugging Face (API gratuita)
4. **Prioridad 4**: Cohere (demo key)
5. **Prioridad 5**: Ollama Local (si est√° instalado)
6. **Prioridad 6**: Local Fallback (siempre disponible)

Si una IA falla por cualquier motivo (l√≠mite de rate, servidor ca√≠do, etc.), el sistema autom√°ticamente prueba la siguiente en 15 segundos o menos.

## üìä Monitoreo del Estado

La aplicaci√≥n incluye un indicador de estado que muestra:
- ‚úÖ Qu√© proveedores est√°n disponibles
- ‚ùå Qu√© proveedores est√°n ca√≠dos
- üîÑ Verificaci√≥n autom√°tica cada 5 minutos
- üìà Prioridad de cada proveedor

## üö® Soluci√≥n de Problemas

### "Todas las IAs fallaron"
- Verifica tu conexi√≥n a internet
- Revisa que las API keys sean correctas
- El sistema siempre tiene Local Fallback como respaldo

### "Respuestas de baja calidad"
- Groq generalmente da las mejores respuestas
- OpenRouter es buena alternativa
- Local Fallback es b√°sico pero siempre funciona

### "Muy lento"
- Groq es el m√°s r√°pido (< 1 segundo)
- Hugging Face puede ser lento en primera carga
- Ollama Local depende de tu hardware

## üí° Consejos

1. **Para mejor rendimiento**: Configura Groq
2. **Para m√°xima disponibilidad**: Configura m√∫ltiples proveedores
3. **Para uso offline**: Instala Ollama Local
4. **Para desarrollo**: OpenRouter funciona sin API key

## üîß Desarrollo

Para agregar un nuevo proveedor de IA:

1. Agrega el proveedor en `lib/ai-solver.ts`
2. Implementa el m√©todo `solveWith[Provider]`
3. Agrega el proveedor al array en `initializeProviders()`
4. Actualiza el componente `AIStatusIndicator`

¬°El sistema autom√°ticamente lo incluir√° en el fallback!